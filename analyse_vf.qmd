---
title: "Analyse_vf"
format: html
editor: visual
---

Introduction : Présentation du modèle conceptuel

Les résultats présentés ci-après sont tirés des travaux de la thèse de doctorat de Yoann Ducoux. L'objectif de l'étude quantitative menée est d'améliorer la compréhension de la structure cognitive qui permet aux candidats de passer de l'observation d'informations à propos d'un employeur à un sentiment favorable envers cet employeur. Par extension, cette étude aide les entreprises à favoriser les chances de réussite du processus de recrutement.

En vertu de la théorie des signaux (Spence, 1973; Stiglitz, 1975) et de son extension la théorie de la signalisation par les tierces parties de la marque employeur (Dineen et al.,2019) nous souhaitons démontrer le rôle central de la crédibilité dans la création de l'image employeur chez le candidat. L'étude présente donc en guise d'informations des signaux, c'est-à-dire des informations qui ne permettent pas au candidat de juger de la qualité de l'entreprise sur la base de ses préférences individuelles mais uniquement à travers la confiance qu'il peut avoir sur le fait que l'entreprise se révelera un employeur satisfaisant au regard de ses propres attentes.

Pour collecter les données, nous avons créé un stimuli qui présente la page carrière d'une entreprise fictive ABCompany. Le stimuli comporte deux variables : la note Glassdoor qui dispose de trois modalités (bonne/neutre/mauvaise) et une enquête de satisfaction des employés de l'entreprise dont la source prend deux modalités (interne/externe). Chaque répondant a donc été exposé à l'un des six scénarios comportant une combinaison de la Note Glassdoor et de la Source de l'enquête de satisfaction.

Les variables qui composent le modèle sont :

-   La Note Glassdoor de l'employeur fictif ABCompany qui varie entre 2,5/5 3,5/5 et 4,5/5.

-   La Source de l'enquête de satisfaction des employés de l'entreprise ABCompany.

-   La congruence des signaux (Note Glassdoor et enquête de satisfaction)

-   La crédibilité des signaux, c'est-à-dire la crédibilité de la Note Glassdoor et la crédibilité de l'enquête de satisfaction

-   La crédibilité de la marque employeur de l'entreprise ABCompany

-   L'image employeur de l'entreprise ABCompany

-   Et deux variables de contrôle : l'âge des répondants et la familiarité avec l'entreprise ABC, c'est-à-dire l'impression de connaitre l'entreprise avant d'avoir vu les signaux que nous leur présentons.

NB : En raison d'un erreur du questionnaire Qualtrics, les réponses des répondants du scénario 1 pour l'échelle de mesure de l'une des variables du modèle (CredE) n'ont pas été enregistrées. Nous avons donc fait une seconde collecte complémentaire, uniquement pour le scénario 1, qui a ensuite servi à imputer les données manquantes de la première collecte. La procédure d'imputation est expliquée dans le document.

```{r}
#| label: conceptual_diagram
#| fig.width: 12
#| fig.height: 8
#| out.width: "100%"
#| echo: false
#| warning: false
#| message: false

# Alternative à DiagrammeR en utilisant ggplot2 qui est plus stable
library(ggplot2)
library(ggtext)
library(grid)

# Définir un thème personnalisé pour le diagramme
theme_diagram <- theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    plot.margin = margin(20, 20, 20, 20)
  )

# Créer un dataframe pour les positions des nœuds
nodes <- data.frame(
  id = c("Note", "Source", "Cong", "CredE", "CredGD", "CredME", "Image"),
  x = c(1, 1, 3, 5, 5, 7.5, 10),  # Augmenter l'espace entre CredME et Image
  y = c(1, 5, 3, 5, 1, 3, 3),
  label = c("Note", "Source", "Cong", "CredE", "CredGD", "CredME", "Image"),
  type = c("observed", "observed", "latent", "latent", "latent", "latent", "latent"),
  width = c(1.4, 1.4, 1.8, 1.8, 1.8, 1.8, 1.8),  # Largeur des formes
  height = c(0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0)   # Hauteur des formes
)

# Créer un dataframe pour les arêtes
edges <- data.frame(
  from = c("Note", "Source", "Note", "Note", "Source", "Cong", "Cong", "CredE", "CredGD", "CredME"),
  to = c("Cong", "Cong", "CredGD", "Image", "CredE", "CredE", "CredGD", "CredME", "CredME", "Image")
)

# Calculer les points de départ et d'arrivée des flèches
edges$x_from <- nodes$x[match(edges$from, nodes$id)]
edges$y_from <- nodes$y[match(edges$from, nodes$id)]
edges$x_to <- nodes$x[match(edges$to, nodes$id)]
edges$y_to <- nodes$y[match(edges$to, nodes$id)]
edges$type_from <- nodes$type[match(edges$from, nodes$id)]
edges$type_to <- nodes$type[match(edges$to, nodes$id)]
edges$width_from <- nodes$width[match(edges$from, nodes$id)]
edges$height_from <- nodes$height[match(edges$from, nodes$id)]
edges$width_to <- nodes$width[match(edges$to, nodes$id)]
edges$height_to <- nodes$height[match(edges$to, nodes$id)]

# Calculer les angles et les points exacts de départ et d'arrivée
for (i in 1:nrow(edges)) {
  # Calculer l'angle de départ
  angle_from_to <- atan2(
    edges$y_to[i] - edges$y_from[i],
    edges$x_to[i] - edges$x_from[i]
  )
  
  # Calculer l'angle d'arrivée
  angle_to_from <- atan2(
    edges$y_from[i] - edges$y_to[i],
    edges$x_from[i] - edges$x_to[i]
  )
  
  # Cas spécial pour Source-Cong et Note-Cong (même x, différent y)
  if ((edges$from[i] == "Source" && edges$to[i] == "Cong") || 
      (edges$from[i] == "Note" && edges$to[i] == "Cong")) {
    # Point de départ (à droite du rectangle)
    edges$x_start[i] <- edges$x_from[i] + edges$width_from[i]/2
    edges$y_start[i] <- edges$y_from[i]
    
    # Point d'arrivée (à gauche de l'ovale)
    edges$x_end[i] <- edges$x_to[i] - edges$width_to[i]/2
    edges$y_end[i] <- edges$y_to[i]
  } else {
    # Calculer le point de départ
    if (edges$type_from[i] == "observed") {
      # Pour les rectangles
      if (abs(cos(angle_from_to)) > abs(sin(angle_from_to))) {
        # Sortie horizontale
        edges$x_start[i] <- edges$x_from[i] + sign(cos(angle_from_to)) * edges$width_from[i]/2
        edges$y_start[i] <- edges$y_from[i] + tan(angle_from_to) * sign(cos(angle_from_to)) * edges$width_from[i]/2
      } else {
        # Sortie verticale
        edges$x_start[i] <- edges$x_from[i] + tan(pi/2 - angle_from_to) * sign(sin(angle_from_to)) * edges$height_from[i]/2
        edges$y_start[i] <- edges$y_from[i] + sign(sin(angle_from_to)) * edges$height_from[i]/2
      }
    } else {
      # Pour les ovales
      edges$x_start[i] <- edges$x_from[i] + cos(angle_from_to) * edges$width_from[i]/2
      edges$y_start[i] <- edges$y_from[i] + sin(angle_from_to) * edges$height_from[i]/2
    }
    
    # Calculer le point d'arrivée
    if (edges$type_to[i] == "observed") {
      # Pour les rectangles
      if (abs(cos(angle_to_from)) > abs(sin(angle_to_from))) {
        # Entrée horizontale
        edges$x_end[i] <- edges$x_to[i] + sign(cos(angle_to_from)) * edges$width_to[i]/2
        edges$y_end[i] <- edges$y_to[i] + tan(angle_to_from) * sign(cos(angle_to_from)) * edges$width_to[i]/2
      } else {
        # Entrée verticale
        edges$x_end[i] <- edges$x_to[i] + tan(pi/2 - angle_to_from) * sign(sin(angle_to_from)) * edges$height_to[i]/2
        edges$y_end[i] <- edges$y_to[i] + sign(sin(angle_to_from)) * edges$height_to[i]/2
      }
    } else {
      # Pour les ovales
      edges$x_end[i] <- edges$x_to[i] + cos(angle_to_from) * edges$width_to[i]/2
      edges$y_end[i] <- edges$y_to[i] + sin(angle_to_from) * edges$height_to[i]/2
    }
  }
}

# Créer le diagramme
p <- ggplot() +
  # Ajouter les rectangles pour les variables observées
  geom_rect(data = subset(nodes, type == "observed"), 
            aes(xmin = x - width/2, xmax = x + width/2, 
                ymin = y - height/2, ymax = y + height/2),
            fill = "lightblue", color = "darkblue", alpha = 0.7, size = 1) +
  
  # Ajouter les ovales pour les variables latentes
  geom_polygon(data = do.call(rbind, lapply(which(nodes$type == "latent"), function(i) {
    theta <- seq(0, 2*pi, length.out = 100)
    data.frame(
      id = nodes$id[i],
      x = nodes$x[i] + (nodes$width[i]/2)*cos(theta),
      y = nodes$y[i] + (nodes$height[i]/2)*sin(theta)
    )
  })), aes(x = x, y = y, group = id), 
  fill = "lightyellow", color = "darkgoldenrod", alpha = 0.7, size = 1) +
  
  # Ajouter les étiquettes des nœuds
  geom_text(data = nodes, aes(x = x, y = y, label = label), 
            fontface = "bold", size = 4) +
  
  # Ajouter les flèches entre les nœuds
  geom_segment(data = edges, 
               aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
               arrow = arrow(length = unit(0.3, "cm"), type = "closed"),
               color = "darkgrey", size = 1) +
  
  # Appliquer le thème personnalisé
  theme_diagram +
  
  # Définir les limites du graphique
  xlim(0, 11) +
  ylim(0, 6) +
  
  # Ajouter un titre
  ggtitle("Modèle conceptuel") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

# Afficher le diagramme
p
```

## Hypothèses de recherche

Sur la base de notre cadre conceptuel, nous formulons les hypothèses suivantes concernant les relations entre la note d'employeur, sa source, et l'image employeur de l'entreprise:

### Relations directes

**H1**: La note Glassdoor de l'employeur (Note) influence positivement l'image employeur (Image).

**H2**: La note Glassdoor de l'employeur (Note) influence positivement la congruence des signaux (Cong).

**H3**: La source de l'enquête (Source) influence positivement la congruence des signaux (Cong).

**H4**: La note Glassdoor de l'employeur (Note) influence la crédibilité perçue de la note Glassdoor de l'employeur (CredGD).

**H5**: La source de l'enquête (Source) influence la crédibilité perçue de l'enquête (CredE).

**H6**: La congruence des signaux (Cong) influence positivement la crédibilité des signaux. - H6a: La congruence des signaux (Cong) influence positivement crédibilité de l'enquête (CredE). - H6b: La congruence des signaux (Cong) influence positivement crédibilité de la note Glassdoor (CredGD).

**H7**: La crédibilité de la marque employeur (CredME) influence positivement l'image employeur (Image).

**H8**: La crédibilité des signaux influence la crédibilité de la marque employeur(CredME). - H8a: La crédibilité de la note Glassdoor (CredGD) influence positivement crédibilité de la marque employeur (CredME). - H8b: La crédibilité de l'enquête (CredE) influence positvement crédibilité de la marque employeur (CredME).

### Médiations

**H9**: Les effets de la source de l'enquête sur l'image employeur sont médiatisés par une chaîne de médiation (médiation séquentielle) impliquant dans l'ordre la congruence des signaux, la crédibilité des signaux et la crédibilité de la marque employeur. - H9a: La séquence de médiateurs congruence des signaux, crédibilité de la note Glassdoor, crédibilité de la marque employeur médiatise la relation entre la source de l'enquête et l'image employeur. - H9b: La séquence de médiateurs congruence des signaux, crédibilité de l'enquête, crédibilité de la marque employeur médiatise la relation entre la source de l'enquête et l'image employeur.

**H10**: Les effets de la source de l'enquête sur l'image employeur sont médiatisés par une chaîne de médiation (médiation séquentielle) impliquant dans l'ordre la crédibilité de l'enquête et la crédibilité de la marque employeur.

**H11**: Les effets de la note Glassdoor sur l'image employeur sont médiatisés par une chaîne de médiation (médiation séquentielle) impliquant dans l'ordre la crédibilité de la note Glassdoor et la crédibilité de la marque employeur.

**H12**: Les effets de la note sur l'image employeur sont médiatisés par une chaîne de médiation (médiation séquentielle) impliquant dans l'ordre la congruence des signaux, la crédibilité des signaux et la crédibilité de la marque employeur. - H12a: La séquence de médiateurs congruence des signaux, crédibilité de la note Glassdoor, crédibilité de la marque employeur médiatise la relation entre la note Glassdoor et l'image employeur. - H12b: La séquence de médiateurs congruence des signaux, crédibilité de l'enquête, crédibilité de la marque employeur médiatise la relation entre la note Glassdoor et l'image employeur.

```{r}
#| label: setup_and_libraries
#| warning: false
#| message: false
library(dplyr)
library(mice)
library(VIM)
library(mitools)
library(knitr)
library(lavaan)
library(semTools)
library(haven)
library(sjlabelled)
library(sjmisc)
library(psych)
library(ggplot2)

# Importation des données
collecte_1 <- read_sav("data/collecte_1.sav")
collecte_2 <- read_sav("data/collecte_2.sav")

# Fusion des données
merged_df <- bind_rows(collecte_1, collecte_2)

# Conversion des variables labellisées en types R standard
merged_df <- haven::zap_labels(merged_df)
```

# 2. Nettoyage et préparation des données

```{r}
#| label: data_cleaning
#| results: "hide"
#| warning: false
#| message: false
# 1. Filtrage sur la complétion
merged_df <- merged_df %>%
  filter(Progress == 100)           # Progression complète

# 2. Exclusion des temps aberrants
q_lower <- quantile(merged_df$Duration__in_seconds_, 0.05, na.rm = TRUE)
q_upper <- quantile(merged_df$Duration__in_seconds_, 0.95, na.rm = TRUE)
merged_df <- subset(merged_df,
  Duration__in_seconds_ >= q_lower & 
  Duration__in_seconds_ <= q_upper)

# 3. Filtrage sur le check d'attention
merged_df <- merged_df %>%
  filter(Informations_site_5 == 6)  # Check d'attention

# 4. Création des variables scénario, Note et Source
merged_df <- merged_df %>%
  mutate(
    scenario = case_when(
      !is.na(A25_baro_chrono_Page_Submit)   ~ 1,
      !is.na(A25extchrono_Page_Submit)      ~ 2,
      !is.na(A35baro_chrono_Page_Submit)    ~ 3,
      !is.na(A35extchrono_Page_Submit)      ~ 4,
      !is.na(A45_baro_chrono_Page_Submit)   ~ 5,
      !is.na(A45extchrono_Page_Submit)      ~ 6,
      TRUE                                  ~ NA_real_
    ),
    Note = case_when(
      scenario %in% c(1, 2) ~ 2.5,  # Codé comme 1
      scenario %in% c(3, 4) ~ 3.5,  # Codé comme 2
      scenario %in% c(5, 6) ~ 4.5,  # Codé comme 3
      TRUE                  ~ NA_real_
    ),
    Source = case_when(
      scenario %in% c(1, 3, 5) ~ 1,
      scenario %in% c(2, 4, 6) ~ 2,
      TRUE                     ~ NA_real_
    )
  )

# 5. Conversion en numérique de toutes les variables
merged_df <- merged_df %>%
  mutate(
    across(c(starts_with("congruence_signaux_"),
             starts_with("Cr_dibilit__ABC_"),
             starts_with("Cr_dibilit__GD_"),
             starts_with("Image_employeur_"),
             starts_with("Familiarit__ABC_"),
             starts_with("Cr_dibilit__baro_"),
             starts_with("cr_dibilit__baro_ext_"),
             age, genre), as_numeric)
  )

# 6. Recodage de congruence_signaux_3 (item inversé)
merged_df <- merged_df %>%
  mutate(
    congruence_signaux_3 = case_when(
      congruence_signaux_3 %in% 1:7 ~ 8 - congruence_signaux_3,
      TRUE ~ congruence_signaux_3
    )
  )

# 7. Création de age_num
merged_df$age_num <- as.numeric(gsub("[^0-9]", "", merged_df$age))

# 8. Création des scores composites (sauf CredE qui sera créé après imputation)
merged_df <- merged_df %>%
  mutate(
    Cong = rowMeans(across(starts_with("congruence_signaux_")), na.rm = TRUE),
    CredME = rowMeans(across(starts_with("Cr_dibilit__ABC_")), na.rm = TRUE),
    CredGD = rowMeans(across(starts_with("Cr_dibilit__GD_")), na.rm = TRUE),
    Image = rowMeans(across(starts_with("Image_employeur_")), na.rm = TRUE),
    FAM = rowMeans(across(starts_with("Familiarit__ABC_")), na.rm = TRUE)
  )

# 9. Variables à conserver pour l'analyse finale
vars_to_keep <- c(
    # Variables de base
    "Note", "Source", "scenario", 
    # Scores composites
    "Cong", "CredGD", "CredME", "Image", "FAM",
    # Variables de contrôle
    "genre", "age_num",
    # Items de crédibilité pour imputation
    "Cr_dibilit__baro_int_1", "Cr_dibilit__baro_int_2", 
    "Cr_dibilit__baro_int_3", "Cr_dibilit__baro_int_4",
    "cr_dibilit__baro_ext_1", "cr_dibilit__baro_ext_2",
    "cr_dibilit__baro_ext_3", "cr_dibilit__baro_ext_4"
)

# 10. Préparation des sous-ensembles pour l'imputation
# Données pour la fusion (scénarios 2 à 6)
subset_for_merging <- merged_df %>%
  filter(scenario %in% c(2:6)) %>%
  select(all_of(vars_to_keep))

# Données complètes (tous scénarios, sans données manquantes)
subset_noDM <- merged_df %>%
  filter(
    scenario != 1 | 
    (scenario == 1 & !is.na(Cr_dibilit__baro_int_1))
  ) %>%
  select(all_of(vars_to_keep))

# Variables à imputer spécifiquement pour le scénario 1
vars_to_impute <- c(
    # Variables de base
    "Note", "Source", "scenario", 
    # Scores composites
    "Cong", "CredGD", "CredME", "Image", "FAM",
    # Variables de contrôle
    "genre", "age_num",
    # Uniquement les items de crédibilité interne (pertinents pour scénario 1)
    "Cr_dibilit__baro_int_1", "Cr_dibilit__baro_int_2", 
    "Cr_dibilit__baro_int_3", "Cr_dibilit__baro_int_4"
)

# Données du scénario 1 pour imputation
subset_scenario1 <- merged_df %>%
  filter(scenario == 1) %>%
  select(all_of(vars_to_impute))

# Conversion des variables labellisées en types R standard
subset_scenario1 <- subset_scenario1 %>%
  mutate(across(everything(), function(x) {
    if(inherits(x, "haven_labelled")) {
      return(as.numeric(x))
    } else {
      return(x)
    }
  }))
```

## Imputation multiple et règle de Rubin

L'imputation multiple est une méthode statistique robuste pour traiter les données manquantes. Contrairement à l'imputation simple qui remplace chaque valeur manquante par une seule estimation, l'imputation multiple crée plusieurs jeux de données complets (généralement entre 5 et 20) où les valeurs manquantes sont remplacées par différentes estimations plausibles. Cette approche permet de capturer l'incertitude liée à l'imputation.

Dans notre étude, nous utilisons deux méthodes d'imputation différentes :

1.  **Predictive Mean Matching (PMM)** : Cette méthode impute les valeurs manquantes en utilisant des valeurs observées provenant d'autres observations ayant des caractéristiques similaires. Elle préserve la distribution des données observées et est particulièrement adaptée aux variables continues.

2.  **Random Forest (RF)** : Cette méthode utilise des algorithmes d'apprentissage automatique pour prédire les valeurs manquantes en fonction des relations complexes entre les variables. Elle est capable de capturer des relations non linéaires et des interactions.

Pour combiner les résultats des analyses effectuées sur chaque jeu de données imputé, nous appliquons la **règle de Rubin** (1987), qui permet d'obtenir des estimations non biaisées des paramètres et de leurs erreurs standard. Cette règle stipule que :

-   L'estimation combinée d'un paramètre est la moyenne des estimations obtenues sur chaque jeu imputé.
-   La variance totale associée à cette estimation est la somme de deux composantes :
    1.  La variance intra-imputation (moyenne des variances estimées dans chaque jeu)
    2.  La variance inter-imputation (variance des estimations entre les jeux), multipliée par un facteur d'ajustement (1 + 1/m, où m est le nombre d'imputations)

Cette approche nous permet d'obtenir des résultats plus fiables en tenant compte à la fois de l'incertitude liée à l'échantillonnage et de celle liée à l'imputation des données manquantes.

```{r}
#| label: imputation
#| results: "hide"
#| warning: false
#| message: false
# 11. Configuration pour l'imputation
predictors <- c("Image", "Cong", "CredME", "CredGD")

# 12. Imputation
# PMM
imp_pmm <- mice(subset_scenario1, m = 10, method = "pmm", 
                predictorMatrix = quickpred(subset_scenario1, 
                                         include = predictors),
                seed = 123)

# RF
imp_rf <- mice(subset_scenario1, m = 10, method = "rf",
               predictorMatrix = quickpred(subset_scenario1, 
                                        include = predictors),
               seed = 123)

# 13. Création des jeux de données finaux avec fusion des items de crédibilité
create_final_datasets <- function(imp, subset_for_merging) {
  datasets <- vector("list", imp$m)
  for(i in 1:imp$m) {
    # Récupération des données imputées du scénario 1
    imputed_data <- complete(imp, action = i)
    
    # Fusion avec les autres scénarios
    combined_data <- bind_rows(imputed_data, subset_for_merging) %>%
      arrange(scenario)
    
    # Création des variables CredE fusionnées
    combined_data <- combined_data %>%
      mutate(
        CredE1 = case_when(
          Source == 1 ~ Cr_dibilit__baro_int_1,
          Source == 2 ~ cr_dibilit__baro_ext_1
        ),
        CredE2 = case_when(
          Source == 1 ~ Cr_dibilit__baro_int_2,
          Source == 2 ~ cr_dibilit__baro_ext_2
        ),
        CredE3 = case_when(
          Source == 1 ~ Cr_dibilit__baro_int_3,
          Source == 2 ~ cr_dibilit__baro_ext_3
        ),
        CredE4 = case_when(
          Source == 1 ~ Cr_dibilit__baro_int_4,
          Source == 2 ~ cr_dibilit__baro_ext_4
        ),
        CredE = rowMeans(cbind(CredE1, CredE2, CredE3, CredE4), na.rm = TRUE)
      )
    
    datasets[[i]] <- combined_data
  }
  return(datasets)
}

# Création des trois jeux de données finaux
subset_pmm <- create_final_datasets(imp_pmm, subset_for_merging)
subset_rf <- create_final_datasets(imp_rf, subset_for_merging)

# Application de la même logique aux données complètes
subset_noDM <- subset_noDM %>%
  mutate(
    CredE1 = case_when(
      Source == 1 ~ Cr_dibilit__baro_int_1,
      Source == 2 ~ cr_dibilit__baro_ext_1
    ),
    CredE2 = case_when(
      Source == 1 ~ Cr_dibilit__baro_int_2,
      Source == 2 ~ cr_dibilit__baro_ext_2
    ),
    CredE3 = case_when(
      Source == 1 ~ Cr_dibilit__baro_int_3,
      Source == 2 ~ cr_dibilit__baro_ext_3
    ),
    CredE4 = case_when(
      Source == 1 ~ Cr_dibilit__baro_int_4,
      Source == 2 ~ cr_dibilit__baro_ext_4
    ),
    CredE = rowMeans(cbind(CredE1, CredE2, CredE3, CredE4), na.rm = TRUE)
  )

# 14. Vérification des données manquantes dans chaque jeu
check_missing <- function(data, name) {
  cat("\nVérification des valeurs manquantes dans", name, ":\n")
  print(sapply(data, function(x) sum(is.na(x))))
}

check_missing(subset_noDM, "subset_noDM")
check_missing(subset_pmm[[1]], "premier jeu PMM")
check_missing(subset_rf[[1]], "premier jeu RF")
```

#Procédure de vérification

## Vérification des manipulations expérimentales

Avant de procéder aux analyses principales, nous vérifions l'efficacité des manipulations expérimentales entre les différents scénarios. Ces analyses ANOVA nous permettent de confirmer que les participants ont bien perçu les différences entre les conditions expérimentales, notamment concernant les informations présentées sur Glassdoor et sur le site de l'entreprise.

```{r}
#| label: Checks
#| results: "hide"
# Définition des items à vérifier
items_check <- c("Info_Glassdoor_1", "Info_Glassdoor_2", 
                 "Info_Glassdoor_3", "Info_Glassdoor_4", 
                 "Informations_site_1", "Informations_site_2",
                 "Informations_site_3", "Informations_site_4", "Informations_site_5")

# Analyse ANOVA pour chaque item par scénario
for (item in items_check) {
  cat("\n=== ANOVA for", item, "by scenario ===\n")
  frm <- as.formula(paste(item, "~ factor(scenario)"))
  mod <- aov(frm, data=merged_df)
  print(summary(mod))
}
```

### Interprétation des résultats des ANOVAs

L'analyse des résultats des ANOVAs nous permet d'évaluer systématiquement l'efficacité de nos manipulations expérimentales à travers les différents scénarios :

#### Manipulation de la note et source d'évaluation (Glassdoor)

1.  **Info_Glassdoor_1** - *"La note sur l'employeur ABCompany qui apparaît sur son site carrière émane de Glassdoor"*
    -   Non significatif : F(5,399) = 1.695, p = 0.135
    -   La reconnaissance de Glassdoor comme source est similaire dans tous les scénarios, suggérant une attribution claire de la source indépendamment des conditions expérimentales.
2.  **Info_Glassdoor_2** - *"Le résultat de cette évaluation est indépendant de l'entreprise ABCompany"*
    -   Non significatif : F(5,399) = 1.449, p = 0.206
    -   La perception d'indépendance de l'évaluation reste constante à travers les scénarios, confirmant l'uniformité de cette dimension dans notre dispositif expérimental.
3.  **Info_Glassdoor_3** - *"D'après les informations de Glassdoor, l'entreprise ABCompany est un bon employeur"*
    -   Très significatif : F(5,399) = 76.96, p \< 0.001
    -   Cette différence hautement significative confirme que notre manipulation de la note (2.5, 3.5, 4.5) a été correctement perçue par les participants, validant le succès de cette manipulation cruciale.
4.  **Info_Glassdoor_4** - *"D'après les informations de Glassdoor, les employés de ABC recommandent de travailler pour cet employeur"*
    -   Très significatif : F(5,399) = 35.29, p \< 0.001
    -   Cette différence significative renforce la confirmation de l'efficacité de notre manipulation des évaluations, montrant que les participants perçoivent des niveaux différents de recommandation selon les scénarios.

#### Perception du site carrière

1.  **Informations_site_1** - *"L'entreprise ABCompany est à l'origine de ce site internet"*
    -   Non significatif : F(5,399) = 0.947, p = 0.451
    -   La perception de l'entreprise comme source du site reste constante entre les scénarios, assurant une base homogène pour l'interprétation des autres variables.
2.  **Informations_site_2** - *"Le site que j'ai vu est le site carrière de l'entreprise ABCompany"*
    -   Non significatif : F(5,399) = 0.494, p = 0.781
    -   L'identification uniforme du site comme site carrière à travers les scénarios assure la cohérence de notre environnement expérimental.
3.  **Informations_site_3** - *"Ce site carrière fait apparaître un taux de recommandation de l'entreprise ABCompany comme employeur"*
    -   Non significatif : F(5,399) = 1.184, p = 0.316
    -   La visibilité perçue du taux de recommandation est constante entre les scénarios, confirmant la standardisation de cet élément dans notre design.
4.  **Informations_site_4** - *"Ce site carrière fait apparaître une note sur ABCompany comme employeur"*
    -   Non significatif : F(5,399) = 1.659, p = 0.144
    -   La perception de la présence d'une note est homogène à travers les différentes conditions, soutenant la cohérence de notre présentation expérimentale.
5.  **Informations_site_5** - *Check d'attention*
    -   Non significatif : F(5,399) = 1.364, p = 0.237
    -   Cette homogénéité est attendue pour un item de vérification d'attention, confirmant la fiabilité de notre filtre d'inclusion des participants.

Cette analyse systématique révèle que nos manipulations expérimentales ont fonctionné comme prévu. Spécifiquement, les différences significatives observées pour les items 3 et 4 de Glassdoor confirment que la manipulation de la note de l'employeur a été efficacement perçue, tandis que l'homogénéité des autres perceptions atteste de la standardisation appropriée des éléments non manipulés. Ces vérifications valident la robustesse de notre design expérimental et renforcent la fiabilité des analyses subséquentes.

Ces vérifications confirment que nos manipulations expérimentales ont fonctionné comme prévu, ce qui nous permet de procéder avec confiance aux analyses psychométriques et structurelles suivantes.

# 3. Analyses psychométriques

```{r}
#| label: analyses_psychometriques

# Fonction pour les analyses psychométriques complètes
analyse_psychometrique <- function(data, nom_echelle, items) {
  cat("\n=== Analyse psychométrique de", nom_echelle, "===\n")
  
  # Extraction des données de l'échelle
  data_echelle <- data[, items, drop = FALSE]
  complete_cases <- complete.cases(data_echelle)
  data_complete <- data_echelle[complete_cases, , drop = FALSE]
  
  # 1. Statistiques descriptives
  cat("\n1. Statistiques descriptives :\n")
  print(summary(data_echelle))
  
  # 2. Tests de normalité et adéquation
  cat("\n2. Tests de normalité et adéquation :\n")
  tryCatch({
    # KMO
    mat_cor <- cor(data_complete)
    kmo <- KMO(mat_cor)
    cat("- KMO =", round(kmo$MSA, 3), "\n")
    
    # Test de Bartlett
    n <- nrow(data_complete)
    bartlett <- cortest.bartlett(mat_cor, n = n)
    cat("- Test de Bartlett : chi2 =", round(bartlett$chisq, 2),
        ", ddl =", bartlett$df,
        ", p =", format.pval(bartlett$p.value), "\n")
  }, error = function(e) {
    cat("Impossible de calculer KMO/Bartlett:", conditionMessage(e), "\n")
  })
  
  # 3. Fiabilité (Alpha de Cronbach)
  cat("\n3. Fiabilité :\n")
  tryCatch({
    alpha_res <- psych::alpha(data_echelle)
    cat(sprintf("- Alpha de Cronbach = %.3f\n", alpha_res$total$raw_alpha))
    cat("\nStatistiques par item si supprimé :\n")
    print(round(alpha_res$alpha.drop, 3))
    cat("\nCorrélations item-total :\n")
    print(round(alpha_res$item.stats, 3))
  }, error = function(e) {
    cat("- Impossible de calculer l'alpha de Cronbach:", conditionMessage(e), "\n")
    alpha_res <- list(total = list(raw_alpha = NA))
  })
  
  # 4. Analyse factorielle
  cat("\n4. Analyse factorielle :\n")
  tryCatch({
    if(sum(complete_cases) > 1) {
      # Test du nombre de facteurs
      nfactors <- fa.parallel(data_complete, fa = "fa", fm = "ml")
      cat("\nNombre de facteurs suggéré :", nfactors$nfact, "\n")
      
      # Analyse factorielle
      fa_res <- fa(data_complete, nfactors = 1, fm = "ml", rotate = "none")
      cat("\n- Loadings :\n")
      print(round(fa_res$loadings, 3))
      cat("\n- Communalités :\n")
      print(round(fa_res$communality, 3))
      cat(sprintf("\n- Variance expliquée : %.2f%%\n", 
                fa_res$Vaccounted[2,1] * 100))
      
      # Résidus
      cat("\n- Résidus :\n")
      residuals <- factor.residuals(mat_cor, fa_res$loadings)
      cat("RMSR =", sqrt(mean(residuals[lower.tri(residuals)]^2)), "\n")
    } else {
      cat("Pas assez de données complètes pour l'analyse factorielle\n")
      fa_res <- NULL
    }
  }, error = function(e) {
    cat("Erreur dans l'analyse factorielle:", conditionMessage(e), "\n")
    fa_res <- NULL
  })
  
  return(list(
    normalite = list(shapiro = NA,
                    skew = sapply(data_echelle, skew),
                    kurtosis = sapply(data_echelle, kurtosi)),
    kmo = if(exists("kmo")) kmo$MSA else NA,
    bartlett = if(exists("bartlett")) bartlett else NA,
    alpha = alpha_res$total$raw_alpha,
    fa = fa_res
  ))
}

# Analyses psychométriques pour les échelles avec items dans merged_df
# CredGD
items_CredGD <- paste0("Cr_dibilit__GD_", 1:4)
analyse_CredGD <- analyse_psychometrique(merged_df, "CredGD", items_CredGD)

# CredME
items_CredME <- paste0("Cr_dibilit__ABC_", 1:3)
analyse_CredME <- analyse_psychometrique(merged_df, "CredME", items_CredME)

# Congruence
items_Cong <- paste0("congruence_signaux_", 1:3)
analyse_Cong <- analyse_psychometrique(merged_df, "Congruence", items_Cong)

# Image
items_Image <- paste0("Image_employeur_", 1:3)
analyse_Image <- analyse_psychometrique(merged_df, "Image", items_Image)

# FAM
items_FAM <- paste0("Familiarit__ABC_", 1:3)
analyse_FAM <- analyse_psychometrique(merged_df, "Familiarité", items_FAM)

# Analyse de CredE sur subset_noDM car les items sont créés après imputation
items_CredE <- c("CredE1", "CredE2", "CredE3", "CredE4")
analyse_CredE <- analyse_psychometrique(subset_noDM, "CredE", items_CredE)
```

# Analyses psychométriques sur données imputées

```{r}
#| label: analyses_psychometriques_imputed

# Fonction pour analyser CredE sur les jeux imputés
analyse_CredE_imputed <- function(imp_datasets, nom_echelle = "CredE (imputé)", items = c("CredE1", "CredE2", "CredE3", "CredE4")) {
  cat("\n=== Analyse psychométrique de", nom_echelle, "sur données imputées ===\n")
  
  # Initialisation des résultats
  alphas <- numeric(length(imp_datasets))
  kmos <- numeric(length(imp_datasets))
  variances <- numeric(length(imp_datasets))
  
  # Analyse sur chaque jeu imputé
  for (i in 1:length(imp_datasets)) {
    data <- imp_datasets[[i]]
    
    # Extraction des données de l'échelle
    data_echelle <- data[, items, drop = FALSE]
    complete_cases <- complete.cases(data_echelle)
    data_complete <- data_echelle[complete_cases, , drop = FALSE]
    
    # Alpha de Cronbach
    alpha_res <- psych::alpha(data_echelle)
    alphas[i] <- alpha_res$total$raw_alpha
    
    # KMO et analyse factorielle
    if(sum(complete_cases) > 1) {
      mat_cor <- cor(data_complete)
      kmo_res <- KMO(mat_cor)
      kmos[i] <- kmo_res$MSA
      
      fa_res <- fa(data_complete, nfactors = 1, fm = "ml", rotate = "none")
      variances[i] <- fa_res$Vaccounted[2,1] * 100
    }
  }
  
  # Résultats combinés
  cat("\nRésultats combinés sur", length(imp_datasets), "jeux imputés :\n")
  cat("- Alpha de Cronbach moyen =", round(mean(alphas, na.rm = TRUE), 3), "\n")
  cat("- KMO moyen =", round(mean(kmos, na.rm = TRUE), 3), "\n")
  cat("- Variance expliquée moyenne =", round(mean(variances, na.rm = TRUE), 2), "%\n")
  
  return(list(
    alpha_mean = mean(alphas, na.rm = TRUE),
    kmo_mean = mean(kmos, na.rm = TRUE),
    variance_mean = mean(variances, na.rm = TRUE)
  ))
}

# Analyse de CredE sur les jeux imputés PMM
analyse_CredE_pmm <- analyse_CredE_imputed(subset_pmm)

# Analyse de CredE sur les jeux imputés RF
analyse_CredE_rf <- analyse_CredE_imputed(subset_rf)
```

## Synthèse des analyses psychométriques

Cette section présente les analyses psychométriques des différentes échelles utilisées dans notre étude expérimentale. Ces analyses permettent d'évaluer la qualité et la fiabilité des instruments de mesure avant de procéder aux analyses structurelles.

### Crédibilité de Glassdoor (CredGD)

L'échelle de crédibilité perçue de Glassdoor comprend quatre items évalués sur une échelle de Likert à 7 points.

**Propriétés psychométriques** : - Les scores moyens varient entre 3,77 et 4,25, indiquant une perception modérément positive - L'indice KMO (0,599) est acceptable mais modeste - La fiabilité interne (α = 0,563) est en dessous du seuil conventionnel de 0,70 - L'analyse factorielle suggère une structure à trois facteurs, ce qui est problématique pour une échelle censée mesurer un construit unique - Les saturations factorielles sont hétérogènes (de 0,337 à 0,785) - La variance expliquée est limitée (30,46%)

Cette échelle présente des faiblesses psychométriques qui invitent à interpréter les résultats la concernant avec prudence.

### Crédibilité de la Méthode d'Évaluation (CredME)

Cette échelle à trois items mesure la crédibilité perçue de la méthode d'évaluation utilisée par l'entreprise.

**Propriétés psychométriques** : - Les moyennes des items sont homogènes (entre 3,89 et 4,03) - L'indice KMO (0,723) indique une bonne adéquation des données - Excellente fiabilité interne (α = 0,859) - Structure factorielle unidimensionnelle confirmée - Saturations factorielles élevées (0,752 à 0,891) - Variance expliquée satisfaisante (68,07%)

Cette échelle présente d'excellentes qualités psychométriques, soutenant sa validité et sa fiabilité.

### Congruence des Signaux (Cong)

L'échelle de congruence mesure la cohérence perçue entre les différentes sources d'information.

**Propriétés psychométriques** : - Les scores moyens sont homogènes (entre 3,85 et 4,08) - KMO satisfaisant (0,716) - Bonne fiabilité interne (α = 0,853) - Structure factorielle unidimensionnelle confirmée - Saturations factorielles élevées (0,730 à 0,891) - Variance expliquée satisfaisante (66,94%)

Les propriétés psychométriques de cette échelle sont très satisfaisantes.

### Image de l'Employeur (Image)

Cette échelle à trois items évalue la perception globale de l'image de l'employeur.

**Propriétés psychométriques** : - Scores moyens relativement élevés (entre 4,06 et 4,43) - Excellent KMO (0,762) - Fiabilité interne remarquable (α = 0,927) - Structure factorielle unidimensionnelle confirmée - Saturations factorielles très élevées (0,885 à 0,926) - Variance expliquée excellente (81,05%)

Cette échelle présente des qualités psychométriques exceptionnelles, ce qui en fait une mesure très fiable du concept d'image de l'employeur.

### Familiarité avec l'Entreprise (FAM)

L'échelle de familiarité mesure la connaissance préalable que les participants avaient de l'entreprise.

**Propriétés psychométriques** : - Scores moyens très bas (entre 1,27 et 1,33 sur 7) - Bon KMO (0,750) - Fiabilité interne excellente (α = 0,929) - Structure factorielle unidimensionnelle confirmée - Saturations factorielles très élevées (0,846 à 0,946) - Variance expliquée excellente (82,58%)

Les scores très bas indiquent une faible familiarité préalable avec l'entreprise, ce qui est cohérent avec l'utilisation d'une entreprise fictive ou peu connue dans l'expérimentation. Les excellentes propriétés psychométriques confirment néanmoins la validité de cette mesure.

### Crédibilité de l'Évaluateur (CredE)

Cette échelle mesure la crédibilité perçue de la source d'évaluation (interne ou externe).

**Propriétés psychométriques (sur données complètes)** : - Scores moyens homogènes (entre 3,72 et 3,99) - Bon KMO (0,781) - Fiabilité interne satisfaisante (α = 0,829) - Structure factorielle unidimensionnelle confirmée - Saturations factorielles variables mais acceptables (0,606 à 0,887) - Variance expliquée satisfaisante (58,22%)

**Propriétés sur données imputées** : - Méthode PMM : α = 0,821, KMO = 0,758, variance expliquée = 56,98% - Méthode RF : α = 0,801, KMO = 0,771, variance expliquée = 53,46%

La robustesse des indicateurs psychométriques entre les données originales et les données imputées par deux méthodes distinctes (PMM et RF) confirme la stabilité et la fiabilité de cette échelle.

### Conclusion des analyses psychométriques

La majorité des échelles utilisées dans cette étude présentent d'excellentes propriétés psychométriques, avec des coefficients alpha de Cronbach dépassant 0,80, des indices KMO satisfaisants et des structures factorielles clairement unidimensionnelles. Seule l'échelle de crédibilité de Glassdoor (CredGD) montre des faiblesses, avec une fiabilité limitée et une structure factorielle potentiellement multidimensionnelle.

Les résultats similaires obtenus avec les données imputées (PMM et RF) comparés aux données complètes renforcent la confiance dans la stabilité des mesures utilisées et la pertinence des analyses d'imputation multiple pour traiter les données manquantes.

Ces analyses psychométriques robustes constituent un prérequis essentiel pour les analyses structurelles subséquentes, garantissant que les relations entre variables identifiées dans le modèle SEM reposent sur des mesures fiables et valides.

# Analyse de la normalité des variables du modèle

Avant de procéder aux analyses SEM, il est important d'évaluer la normalité des variables incluses dans notre modèle. Cette étape est cruciale car certaines méthodes d'estimation, comme le Maximum Likelihood (ML), supposent que les données suivent approximativement une distribution normale.

```{r}
#| label: normalite_variables
#| fig.width: 12
#| fig.height: 10

# Fonction pour tester la normalité d'une variable
test_normalite <- function(data, var_name) {
  # Extraction de la variable
  var <- data[[var_name]]
  
  # Test de Shapiro-Wilk
  shapiro_test <- shapiro.test(var)
  
  # Calcul de l'asymétrie (skewness) et de l'aplatissement (kurtosis)
  skew_val <- psych::skew(var)
  kurt_val <- psych::kurtosi(var)
  
  # Retourner les résultats
  return(list(
    variable = var_name,
    shapiro_p = shapiro_test$p.value,
    skewness = skew_val,
    kurtosis = kurt_val,
    normal_shapiro = shapiro_test$p.value > 0.05,
    normal_skew_kurt = abs(skew_val) < 1 && abs(kurt_val) < 2
  ))
}

# Variables à tester
variables_modele <- c("Image", "Cong", "CredE", "CredGD", "CredME")

# Tester la normalité sur les données complètes
resultats_normalite <- lapply(variables_modele, function(var) test_normalite(subset_noDM, var))

# Créer un tableau de résultats détaillé
resultats_df <- do.call(rbind, lapply(resultats_normalite, function(x) {
  data.frame(
    Variable = x$variable,
    Shapiro_p = round(x$shapiro_p, 4),
    Skewness = round(x$skewness, 3),
    Kurtosis = round(x$kurtosis, 3),
    Normal_Shapiro = ifelse(x$normal_shapiro, "Oui", "Non"),
    Normal_Skew_Kurt = ifelse(x$normal_skew_kurt, "Oui", "Non")
  )
}))

# Afficher le tableau détaillé
knitr::kable(resultats_df, 
             caption = "Tests de normalité des variables du modèle",
             col.names = c("Variable", "p-value (Shapiro-Wilk)", "Asymétrie", "Aplatissement", 
                          "Normalité (Shapiro)", "Normalité (Asym./Aplat.)"))

# Statistiques descriptives pour chaque variable
stats_desc <- do.call(rbind, lapply(variables_modele, function(var) {
  data.frame(
    Variable = var,
    Moyenne = round(mean(subset_noDM[[var]], na.rm = TRUE), 3),
    Ecart_type = round(sd(subset_noDM[[var]], na.rm = TRUE), 3),
    Minimum = round(min(subset_noDM[[var]], na.rm = TRUE), 3),
    Q1 = round(quantile(subset_noDM[[var]], 0.25, na.rm = TRUE), 3),
    Mediane = round(median(subset_noDM[[var]], na.rm = TRUE), 3),
    Q3 = round(quantile(subset_noDM[[var]], 0.75, na.rm = TRUE), 3),
    Maximum = round(max(subset_noDM[[var]], na.rm = TRUE), 3)
  )
}))

# Afficher les statistiques descriptives
knitr::kable(stats_desc, 
             caption = "Statistiques descriptives des variables du modèle",
             col.names = c("Variable", "Moyenne", "Écart-type", "Minimum", "Q1", "Médiane", "Q3", "Maximum"))

# Tableau détaillé des tests de normalité univariée
normalite_univariee <- data.frame(
  Variable = variables_modele,
  Moyenne = sapply(variables_modele, function(var) round(mean(subset_noDM[[var]], na.rm = TRUE), 3)),
  Ecart_type = sapply(variables_modele, function(var) round(sd(subset_noDM[[var]], na.rm = TRUE), 3)),
  Skewness = sapply(variables_modele, function(var) round(psych::skew(subset_noDM[[var]]), 3)),
  Kurtosis = sapply(variables_modele, function(var) round(psych::kurtosi(subset_noDM[[var]]), 3)),
  Shapiro_W = sapply(variables_modele, function(var) round(shapiro.test(subset_noDM[[var]])$statistic, 3)),
  Shapiro_p = sapply(variables_modele, function(var) round(shapiro.test(subset_noDM[[var]])$p.value, 4)),
  Conclusion = sapply(variables_modele, function(var) {
    skew <- abs(psych::skew(subset_noDM[[var]])) < 1
    kurt <- abs(psych::kurtosi(subset_noDM[[var]])) < 2
    if(skew && kurt) {
      "Acceptable"
    } else {
      "Non normale"
    }
  })
)

# Afficher le tableau détaillé de normalité univariée
knitr::kable(normalite_univariee, 
             caption = "Analyse détaillée de la normalité univariée",
             col.names = c("Variable", "Moyenne", "Écart-type", "Asymétrie", "Aplatissement", 
                          "Shapiro-W", "p-value", "Conclusion"))

# Visualisation des distributions
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))
for (var in variables_modele) {
  # Histogramme avec courbe de densité
  hist(subset_noDM[[var]], 
       main = paste("Distribution de", var),
       xlab = var, 
       probability = TRUE,
       col = "lightblue",
       border = "white")
  lines(density(subset_noDM[[var]], na.rm = TRUE), col = "darkblue", lwd = 2)
  curve(dnorm(x, mean = mean(subset_noDM[[var]], na.rm = TRUE), 
              sd = sd(subset_noDM[[var]], na.rm = TRUE)), 
        add = TRUE, col = "red", lty = 2, lwd = 2)
  legend("topright", 
         legend = c("Densité observée", "Densité normale théorique"),
         col = c("darkblue", "red"), 
         lty = c(1, 2), 
         lwd = 2,
         cex = 0.7)
  
  # QQ-plot
  qqnorm(subset_noDM[[var]], 
         main = paste("QQ-Plot de", var),
         pch = 19,
         col = "darkblue")
  qqline(subset_noDM[[var]], col = "red", lwd = 2)
}
par(mfrow = c(1, 1))

# Analyse de la normalité multivariée
cat("\n=== Test de normalité multivariée ===\n")
library(MVN)
mvn_result <- mvn(subset_noDM[, variables_modele], mvnTest = "mardia")
print(mvn_result$multivariateNormality)

# Inspecter la structure du résultat pour comprendre le problème
cat("\n=== Structure du résultat du test de Mardia ===\n")
cat("Nombre de lignes dans le résultat:", nrow(mvn_result$multivariateNormality), "\n")
cat("Noms des tests:", as.character(mvn_result$multivariateNormality$Test), "\n")

# Créer le tableau de normalité multivariée en extrayant spécifiquement les lignes pour l'asymétrie et l'aplatissement
mardia_skew_idx <- which(mvn_result$multivariateNormality$Test == "Mardia Skewness")
mardia_kurt_idx <- which(mvn_result$multivariateNormality$Test == "Mardia Kurtosis")

# Tableau récapitulatif de la normalité multivariée
mardia_results <- data.frame(
  Test = c("Asymétrie multivariée (Mardia)", "Aplatissement multivarié (Mardia)"),
  Statistique = as.numeric(as.character(mvn_result$multivariateNormality$Statistic[c(mardia_skew_idx, mardia_kurt_idx)])),
  p_value = as.numeric(as.character(mvn_result$multivariateNormality$`p value`[c(mardia_skew_idx, mardia_kurt_idx)]))
)

# Ajouter la conclusion basée sur les valeurs p numériques
mardia_results$Conclusion <- ifelse(mardia_results$p_value > 0.05, "Normale", "Non normale")

# Formater les valeurs p pour l'affichage
mardia_results$p_value_formatted <- format(mardia_results$p_value, scientific = TRUE, digits = 4)

# Afficher le tableau de normalité multivariée
knitr::kable(mardia_results[, c("Test", "Statistique", "p_value_formatted", "Conclusion")], 
             caption = "Résultats des tests de normalité multivariée",
             col.names = c("Test", "Statistique", "p-value", "Conclusion"))

# Ajouter une interprétation explicite des résultats de Mardia
cat("\n**Interprétation du test de Mardia :**\n")
cat("- Asymétrie multivariée : Statistique =", 
    round(as.numeric(as.character(mvn_result$multivariateNormality$Statistic[mardia_skew_idx])), 3), 
    ", p-value =", 
    format(as.numeric(as.character(mvn_result$multivariateNormality$`p value`[mardia_skew_idx])), scientific = TRUE, digits = 4), 
    "\n")
cat("- Aplatissement multivarié : Statistique =", 
    round(as.numeric(as.character(mvn_result$multivariateNormality$Statistic[mardia_kurt_idx])), 3), 
    ", p-value =", 
    format(as.numeric(as.character(mvn_result$multivariateNormality$`p value`[mardia_kurt_idx])), scientific = TRUE, digits = 4), 
    "\n")
cat("- Conclusion : Les deux tests indiquent une déviation significative de la normalité multivariée (p < 0.05).\n")
```

## Interprétation des tests de normalité

L'analyse de la normalité des variables clés de notre modèle révèle plusieurs points importants :

### Normalité univariée

1.  **Test de Shapiro-Wilk** : Ce test formel de normalité est très sensible aux écarts, surtout avec des échantillons de taille moyenne à grande. Nos résultats montrent que toutes les variables principales (Image, Cong, CredE, CredGD, CredME) s'écartent significativement d'une distribution normale selon ce test (p \< 0.05).

2.  **Asymétrie (Skewness)** : L'asymétrie mesure le degré de déséquilibre de la distribution. Nos variables présentent des valeurs d'asymétrie modérées :

    -   Image : asymétrie négative (-0.350), indiquant une tendance vers des valeurs plus élevées
    -   Cong : quasi-symétrique (-0.114)
    -   CredE : asymétrie négative modérée (-0.723)
    -   CredGD : légère asymétrie négative (-0.205)
    -   CredME : asymétrie négative modérée (-0.539)

    Ces valeurs restent toutes dans la plage acceptable (\|skewness\| \< 1), suggérant des déviations modérées de la normalité.

3.  **Aplatissement (Kurtosis)** : L'aplatissement mesure la concentration des valeurs autour de la moyenne. Nos variables montrent :

    -   Image : distribution légèrement plus aplatie (-0.483) qu'une normale
    -   Cong : distribution légèrement plus aplatie (-0.351)
    -   CredE : distribution légèrement plus pointue (0.473)
    -   CredGD : distribution légèrement plus pointue (0.490)
    -   CredME : distribution plus pointue (1.139), mais toujours dans la plage acceptable

    Ces valeurs restent toutes dans la plage acceptable (\|kurtosis\| \< 2).

### Normalité multivariée

Le test de Mardia indique une déviation significative de la normalité multivariée, avec un coefficient d'asymétrie multivariée de 127.73 (p = 1.76e-12) et un coefficient d'aplatissement multivarié de 7.07 (p = 1.51e-12). Cette déviation est courante dans les données issues de questionnaires utilisant des échelles de Likert.

### Implications pour l'analyse SEM

Bien que nos variables s'écartent de la normalité selon les tests formels, les indicateurs d'asymétrie et d'aplatissement restent dans des plages acceptables. Cette situation est typique des données issues de questionnaires en sciences sociales. Pour tenir compte de ces écarts modérés à la normalité, nous avons utilisé l'estimateur MLR (Maximum Likelihood avec erreurs standard robustes) dans nos analyses SEM, qui est robuste aux violations modérées de l'hypothèse de normalité.

Les histogrammes et QQ-plots confirment visuellement que, malgré des déviations statistiquement significatives, nos distributions restent raisonnablement proches de la normalité, avec des déviations principalement aux extrémités des distributions.

Cette analyse de normalité justifie notre choix méthodologique d'utiliser l'estimateur MLR pour les analyses SEM, offrant ainsi un bon compromis entre précision des estimations et robustesse face aux écarts modérés de normalité observés dans nos données.

# 4. Analyses SEM

## Méthode d'équations structurelles (SEM)

Dans cette section, nous utilisons la modélisation par équations structurelles pour analyser les relations directes et indirectes entre nos variables, conformément au cadre conceptuel présenté dans le diagramme initial. Cette approche nous permet d'évaluer simultanément plusieurs chemins de médiation et de quantifier leurs effets respectifs.

Notre stratégie analytique comprend trois approches comparatives du traitement des données manquantes : 1. Analyse sur cas complets (noDM) : utilisation uniquement des observations sans données manquantes 2. Imputation multiple par Predictive Mean Matching (PMM) : 10 jeux de données imputés 3. Imputation multiple par Random Forest (RF) : également 10 jeux de données imputés

Pour chaque approche, nous utilisons l'estimateur MLR (Maximum Likelihood avec erreurs standard robustes) qui offre une meilleure résistance aux déviations de normalité révélées par les tests de normalité.

```{r}
#| label: analyses_sem

# Préparation des données imputées
prepare_imputed_data <- function(imp_list) {
  lapply(imp_list, function(df) {
    # Vérifier si CredE existe, sinon la créer
    if (!"CredE" %in% names(df)) {
      # Vérifier si les composants existent
      has_components <- all(c("CredE1", "CredE2", "CredE3", "CredE4") %in% names(df))
      
      if (has_components) {
        # Créer CredE à partir des composants existants
        df$CredE <- rowMeans(df[, c("CredE1", "CredE2", "CredE3", "CredE4")], na.rm = TRUE)
        cat("Variable CredE créée à partir des composants existants\n")
      } else {
        # Créer les composants si nécessaire
        if (!"CredE1" %in% names(df) && all(c("Source", "Cr_dibilit__baro_int_1", "cr_dibilit__baro_ext_1") %in% names(df))) {
          df$CredE1 <- ifelse(df$Source == 1, df$Cr_dibilit__baro_int_1, df$cr_dibilit__baro_ext_1)
          cat("Variable CredE1 créée\n")
        }
        
        if (!"CredE2" %in% names(df) && all(c("Source", "Cr_dibilit__baro_int_2", "cr_dibilit__baro_ext_2") %in% names(df))) {
          df$CredE2 <- ifelse(df$Source == 1, df$Cr_dibilit__baro_int_2, df$cr_dibilit__baro_ext_2)
          cat("Variable CredE2 créée\n")
        }
        
        if (!"CredE3" %in% names(df) && all(c("Source", "Cr_dibilit__baro_int_3", "cr_dibilit__baro_ext_3") %in% names(df))) {
          df$CredE3 <- ifelse(df$Source == 1, df$Cr_dibilit__baro_int_3, df$cr_dibilit__baro_ext_3)
          cat("Variable CredE3 créée\n")
        }
        
        if (!"CredE4" %in% names(df) && all(c("Source", "Cr_dibilit__baro_int_4", "cr_dibilit__baro_ext_4") %in% names(df))) {
          df$CredE4 <- ifelse(df$Source == 1, df$Cr_dibilit__baro_int_4, df$cr_dibilit__baro_ext_4)
          cat("Variable CredE4 créée\n")
        }
        
        # Maintenant créer CredE
        if (all(c("CredE1", "CredE2", "CredE3", "CredE4") %in% names(df))) {
          df$CredE <- rowMeans(df[, c("CredE1", "CredE2", "CredE3", "CredE4")], na.rm = TRUE)
          cat("Variable CredE créée à partir des composants nouvellement créés\n")
        } else {
          warning("Impossible de créer CredE: composants manquants")
        }
      }
    }
    
    # Convertir toutes les variables en numérique
    df[] <- lapply(df, as.numeric)
    return(df)
  })
}

subset_pmm_prepared <- prepare_imputed_data(subset_pmm)
subset_rf_prepared <- prepare_imputed_data(subset_rf)

# Convertir Note en numérique dans subset_noDM aussi
subset_noDM$Note <- as.numeric(subset_noDM$Note)

# Modèle SEM avec variables de contrôle
model_with_ctrl <- '
# Equation finale (Image)
Image ~ b1*CredME + b2*Note + b3*FAM + b4*age_num

# Médiations
CredME ~ a1*CredE + a2*CredGD
CredE ~ c1*Cong + c2*Source
CredGD ~ d1*Cong + d2*Note
Cong ~ e1*Note + e2*Source

# Effets indirects
ind1 := e2 * c1 * a1 * b1
ind2 := e2 * d1 * a2 * b1
ind3 := c2 * a1 * b1
ind4 := e1 * d1 * a2 * b1
ind5 := e1 * c1 * a1 * b1
ind6 := d2 * a2 * b1

# Effets totaux
tot_ind := ind1 + ind2 + ind3 + ind4 + ind5 + ind6
tot_effect := tot_ind + b2
'

# 1. ANALYSES SUR DONNÉES COMPLÈTES
cat("\n\n===========================================================")
cat("\n=== RÉSULTATS SEM - DONNÉES COMPLÈTES (noDM) ===\n")
cat("===========================================================\n")

# Ajustement du modèle
fit_noDM <- sem(model_with_ctrl, data = subset_noDM, estimator = "MLR")

# Indices d'ajustement
cat("\n=== INDICES D'AJUSTEMENT ===\n")
fit_indices_noDM <- fitMeasures(fit_noDM)
cat(sprintf("Chi² = %.3f (df=%d, p=%.4f)\n", 
            fit_indices_noDM["chisq"], 
            fit_indices_noDM["df"], 
            fit_indices_noDM["pvalue"]))
cat(sprintf("CFI = %.3f\n", fit_indices_noDM["cfi"]))
cat(sprintf("TLI = %.3f\n", fit_indices_noDM["tli"]))
cat(sprintf("RMSEA = %.3f [%.3f; %.3f]\n", 
            fit_indices_noDM["rmsea"], 
            fit_indices_noDM["rmsea.ci.lower"], 
            fit_indices_noDM["rmsea.ci.upper"]))
cat(sprintf("SRMR = %.3f\n", fit_indices_noDM["srmr"]))

# Coefficients et significativité
cat("\n=== COEFFICIENTS DES CHEMINS ===\n")

# Utiliser standardizedSolution pour obtenir les coefficients standardisés
coefs_noDM <- standardizedSolution(fit_noDM)

# Filtrer les chemins de régression et effets indirects
reg_paths <- coefs_noDM[coefs_noDM$op %in% c("~", ":="), ]

# Chemins de régression directs
cat("\nRégressions directes:\n")
direct_paths <- reg_paths[reg_paths$op == "~", ]
for (i in 1:nrow(direct_paths)) {
  signif <- ""
  if (direct_paths$pvalue[i] < 0.001) signif <- "***"
  else if (direct_paths$pvalue[i] < 0.01) signif <- "**"
  else if (direct_paths$pvalue[i] < 0.05) signif <- "*"
  else if (direct_paths$pvalue[i] < 0.1) signif <- "."
  
  cat(sprintf("%s ~ %s (%.3f, p=%.4f) %s\n", 
              direct_paths$lhs[i], 
              direct_paths$rhs[i], 
              direct_paths$est.std[i], 
              direct_paths$pvalue[i],
              signif))
}

# Effets indirects
cat("\nEffets indirects et totaux:\n")
indirect_paths <- reg_paths[reg_paths$op == ":=", ]
for (i in 1:nrow(indirect_paths)) {
  signif <- ""
  if (indirect_paths$pvalue[i] < 0.001) signif <- "***"
  else if (indirect_paths$pvalue[i] < 0.01) signif <- "**"
  else if (indirect_paths$pvalue[i] < 0.05) signif <- "*"
  else if (indirect_paths$pvalue[i] < 0.1) signif <- "."
  
  cat(sprintf("%s = %.3f (p=%.4f) %s\n", 
              indirect_paths$label[i], 
              indirect_paths$est.std[i], 
              indirect_paths$pvalue[i],
              signif))
}

# 2. ANALYSES SUR DONNÉES IMPUTÉES PMM
cat("\n\n===========================================================")
cat("\n=== RÉSULTATS SEM - IMPUTATION PMM ===\n")
cat("===========================================================\n")

# Ajustement du modèle sur chaque jeu imputé
cat("Ajustement du modèle SEM sur chaque jeu imputé PMM...\n")
fit_pmm_list <- vector("list", length(subset_pmm_prepared))
successful_fits <- 0  # Initialisation de successful_fits

# Ajuster le modèle sur chaque jeu imputé manuellement
for (i in 1:length(subset_pmm_prepared)) {
  tryCatch({
    fit_pmm_list[[i]] <- sem(model_with_ctrl, data = subset_pmm_prepared[[i]], estimator = "MLR")
    cat("Modèle", i, "ajusté avec succès\n")
  }, error = function(e) {
    cat("Erreur lors de l'ajustement du modèle pour le jeu", i, ":", conditionMessage(e), "\n")
    fit_pmm_list[[i]] <- NULL
  })
}

# Vérifier combien de modèles ont été ajustés avec succès
successful_fits <- sum(!sapply(fit_pmm_list, is.null))
cat("Nombre de modèles ajustés avec succès:", successful_fits, "sur", length(subset_pmm_prepared), "\n")

# Si aucun modèle n'a été ajusté avec succès, arrêter ici
if (successful_fits == 0) {
  cat("Aucun modèle n'a pu être ajusté. Vérifiez les données et le modèle.\n")
} else {
  # Extraction manuelle des paramètres pour le pooling
  param_list <- vector("list", successful_fits)
  successful_idx <- which(!sapply(fit_pmm_list, is.null))
  
  for (i in 1:successful_fits) {
    idx <- successful_idx[i]
    # Extraire les paramètres standardisés
    params <- standardizedSolution(fit_pmm_list[[idx]])
    
    # Filtrer pour ne garder que les chemins de régression et effets indirects
    params <- params[params$op %in% c("~", ":="), ]
    
    # Créer un dataframe avec les colonnes nécessaires pour le pooling
    param_df <- data.frame(
      term = paste(params$lhs, params$op, params$rhs),
      estimate = params$est.std,
      std.error = params$se,
      statistic = params$z,
      p.value = params$pvalue
    )
    
    param_list[[i]] <- param_df
  }

  # Vérifier que tous les param_list ont le même nombre de lignes
  nrows <- sapply(param_list, nrow)
  if (length(unique(nrows)) > 1) {
    cat("Attention: Les modèles n'ont pas tous le même nombre de paramètres.\n")
    # Utiliser le nombre minimum de paramètres communs
    min_nrow <- min(nrows)
    param_list <- lapply(param_list, function(x) x[1:min_nrow, ])
  }
  
  # Pooling manuel des résultats
  pooled_results <- vector("list", nrow(param_list[[1]]))
  for (j in 1:nrow(param_list[[1]])) {
    # Extraire les estimations et erreurs standard pour ce paramètre
    estimates <- sapply(param_list, function(x) x$estimate[j])
    std.errors <- sapply(param_list, function(x) x$std.error[j])
    
    # Calculer la moyenne des estimations
    q_bar <- mean(estimates)
    
    # Calculer la variance intra-imputation
    u_bar <- mean(std.errors^2)
    
    # Calculer la variance inter-imputation
    b <- var(estimates)
    
    # Calculer la variance totale selon les règles de Rubin
    t <- u_bar + (1 + 1/successful_fits) * b
    
    # Calculer les degrés de liberté
    df <- (successful_fits - 1) * (1 + u_bar / ((1 + 1/successful_fits) * b))^2
    
    # Calculer la statistique t et la p-value
    t_stat <- q_bar / sqrt(t)
    p_value <- 2 * pt(abs(t_stat), df = df, lower.tail = FALSE)
    
    # Stocker les résultats
    pooled_results[[j]] <- c(
      term = param_list[[1]]$term[j],
      estimate = q_bar,
      std.error = sqrt(t),
      statistic = t_stat,
      p.value = p_value,
      df = df
    )
  }
  
  # Convertir en dataframe
  pooled_df <- do.call(rbind, lapply(pooled_results, function(x) {
    data.frame(
      term = x["term"],
      estimate = as.numeric(x["estimate"]),
      std.error = as.numeric(x["std.error"]),
      statistic = as.numeric(x["statistic"]),
      p.value = as.numeric(x["p.value"]),
      df = as.numeric(x["df"])
    )
  }))
  
  # Affichage des résultats poolés
  cat("\n=== RÉSULTATS POOLÉS (PMM) ===\n")
  
  # Chemins de régression directs
  cat("\nRégressions directes:\n")
  direct_paths_pmm <- pooled_df[grepl(" ~ ", pooled_df$term), ]
  for (i in 1:nrow(direct_paths_pmm)) {
    signif <- ""
    if (direct_paths_pmm$p.value[i] < 0.001) signif <- "***"
    else if (direct_paths_pmm$p.value[i] < 0.01) signif <- "**"
    else if (direct_paths_pmm$p.value[i] < 0.05) signif <- "*"
    else if (direct_paths_pmm$p.value[i] < 0.1) signif <- "."
    
    cat(sprintf("%s (β=%.3f, p=%.4f) %s\n", 
                direct_paths_pmm$term[i],
                direct_paths_pmm$estimate[i], 
                direct_paths_pmm$p.value[i],
                signif))
  }
  
  # Effets indirects
  cat("\nEffets indirects et totaux:\n")
  indirect_paths_pmm <- pooled_df[grepl(" := ", pooled_df$term), ]
  for (i in 1:nrow(indirect_paths_pmm)) {
    signif <- ""
    if (indirect_paths_pmm$p.value[i] < 0.001) signif <- "***"
    else if (indirect_paths_pmm$p.value[i] < 0.01) signif <- "**"
    else if (indirect_paths_pmm$p.value[i] < 0.05) signif <- "*"
    else if (indirect_paths_pmm$p.value[i] < 0.1) signif <- "."
    
    cat(sprintf("%s (β=%.3f, p=%.4f) %s\n", 
                indirect_paths_pmm$term[i],
                indirect_paths_pmm$estimate[i], 
                indirect_paths_pmm$p.value[i],
                signif))
  }

  # Pour PMM - Calcul et affichage des indices d'ajustement moyens
  cat("\n=== INDICES D'AJUSTEMENT MOYENS (PMM) ===\n")
  fit_indices_pmm <- lapply(fit_pmm_list[!sapply(fit_pmm_list, is.null)], fitMeasures)
  fit_indices_pmm_mean <- Reduce("+", fit_indices_pmm) / length(fit_indices_pmm)

  cat(sprintf("Chi² moyen = %.3f (df=%.0f)\n", 
              fit_indices_pmm_mean["chisq"], 
              fit_indices_pmm_mean["df"]))
  cat(sprintf("CFI moyen = %.3f\n", fit_indices_pmm_mean["cfi"]))
  cat(sprintf("TLI moyen = %.3f\n", fit_indices_pmm_mean["tli"]))
  cat(sprintf("RMSEA moyen = %.3f\n", fit_indices_pmm_mean["rmsea"]))
  cat(sprintf("SRMR moyen = %.3f\n", fit_indices_pmm_mean["srmr"]))
}

# 3. ANALYSES SUR DONNÉES IMPUTÉES RF
cat("\n\n===========================================================")
cat("\n=== RÉSULTATS SEM - IMPUTATION RF ===\n")
cat("===========================================================\n")

# Ajustement du modèle sur chaque jeu imputé
cat("Ajustement du modèle SEM sur chaque jeu imputé RF...\n")
fit_rf_list <- vector("list", length(subset_rf_prepared))
successful_fits_rf <- 0  # Initialisation de successful_fits_rf

# Ajuster le modèle sur chaque jeu imputé manuellement
for (i in 1:length(subset_rf_prepared)) {
  tryCatch({
    fit_rf_list[[i]] <- sem(model_with_ctrl, data = subset_rf_prepared[[i]], estimator = "MLR")
    cat("Modèle RF", i, "ajusté avec succès\n")
  }, error = function(e) {
    cat("Erreur lors de l'ajustement du modèle RF pour le jeu", i, ":", conditionMessage(e), "\n")
    fit_rf_list[[i]] <- NULL
  })
}

# Vérifier combien de modèles ont été ajustés avec succès
successful_fits_rf <- sum(!sapply(fit_rf_list, is.null))
cat("Nombre de modèles RF ajustés avec succès:", successful_fits_rf, "sur", length(subset_rf_prepared), "\n")

# Si aucun modèle n'a été ajusté avec succès, arrêter ici
if (successful_fits_rf == 0) {
  cat("Aucun modèle RF n'a pu être ajusté. Vérifiez les données et le modèle.\n")
} else {
  # Extraction manuelle des paramètres pour le pooling
  param_list_rf <- vector("list", successful_fits_rf)
  successful_idx_rf <- which(!sapply(fit_rf_list, is.null))
  
  for (i in 1:successful_fits_rf) {
    idx <- successful_idx_rf[i]
    # Extraire les paramètres standardisés
    params <- standardizedSolution(fit_rf_list[[idx]])
    
    # Filtrer pour ne garder que les chemins de régression et effets indirects
    params <- params[params$op %in% c("~", ":="), ]
    
    # Créer un dataframe avec les colonnes nécessaires pour le pooling
    param_df <- data.frame(
      term = paste(params$lhs, params$op, params$rhs),
      estimate = params$est.std,
      std.error = params$se,
      statistic = params$z,
      p.value = params$pvalue
    )
    
    param_list_rf[[i]] <- param_df
  }

  # Vérifier que tous les param_list ont le même nombre de lignes
  nrows <- sapply(param_list_rf, nrow)
  if (length(unique(nrows)) > 1) {
    cat("Attention: Les modèles RF n'ont pas tous le même nombre de paramètres.\n")
    # Utiliser le nombre minimum de paramètres communs
    min_nrow <- min(nrows)
    param_list_rf <- lapply(param_list_rf, function(x) x[1:min_nrow, ])
  }
  
  # Pooling manuel des résultats
  pooled_results_rf <- vector("list", nrow(param_list_rf[[1]]))
  for (j in 1:nrow(param_list_rf[[1]])) {
    # Extraire les estimations et erreurs standard pour ce paramètre
    estimates <- sapply(param_list_rf, function(x) x$estimate[j])
    std.errors <- sapply(param_list_rf, function(x) x$std.error[j])
    
    # Calculer la moyenne des estimations
    q_bar <- mean(estimates)
    
    # Calculer la variance intra-imputation
    u_bar <- mean(std.errors^2)
    
    # Calculer la variance inter-imputation
    b <- var(estimates)
    
    # Calculer la variance totale selon les règles de Rubin
    t <- u_bar + (1 + 1/successful_fits_rf) * b
    
    # Calculer les degrés de liberté
    df <- (successful_fits_rf - 1) * (1 + u_bar / ((1 + 1/successful_fits_rf) * b))^2
    
    # Calculer la statistique t et la p-value
    t_stat <- q_bar / sqrt(t)
    p_value <- 2 * pt(abs(t_stat), df = df, lower.tail = FALSE)
    
    # Stocker les résultats
    pooled_results_rf[[j]] <- c(
      term = param_list_rf[[1]]$term[j],
      estimate = q_bar,
      std.error = sqrt(t),
      statistic = t_stat,
      p.value = p_value,
      df = df
    )
  }
  
  # Convertir en dataframe
  pooled_df_rf <- do.call(rbind, lapply(pooled_results_rf, function(x) {
    data.frame(
      term = x["term"],
      estimate = as.numeric(x["estimate"]),
      std.error = as.numeric(x["std.error"]),
      statistic = as.numeric(x["statistic"]),
      p.value = as.numeric(x["p.value"]),
      df = as.numeric(x["df"])
    )
  }))
  
  # Affichage des résultats poolés
  cat("\n=== RÉSULTATS POOLÉS (RF) ===\n")
  
  # Chemins de régression directs
  cat("\nRégressions directes:\n")
  direct_paths_rf <- pooled_df_rf[grepl(" ~ ", pooled_df_rf$term), ]
  for (i in 1:nrow(direct_paths_rf)) {
    signif <- ""
    if (direct_paths_rf$p.value[i] < 0.001) signif <- "***"
    else if (direct_paths_rf$p.value[i] < 0.01) signif <- "**"
    else if (direct_paths_rf$p.value[i] < 0.05) signif <- "*"
    else if (direct_paths_rf$p.value[i] < 0.1) signif <- "."
    
    cat(sprintf("%s (β=%.3f, p=%.4f) %s\n", 
                direct_paths_rf$term[i],
                direct_paths_rf$estimate[i], 
                direct_paths_rf$p.value[i],
                signif))
  }
  
  # Effets indirects
  cat("\nEffets indirects et totaux:\n")
  indirect_paths_rf <- pooled_df_rf[grepl(" := ", pooled_df_rf$term), ]
  for (i in 1:nrow(indirect_paths_rf)) {
    signif <- ""
    if (indirect_paths_rf$p.value[i] < 0.001) signif <- "***"
    else if (indirect_paths_rf$p.value[i] < 0.01) signif <- "**"
    else if (indirect_paths_rf$p.value[i] < 0.05) signif <- "*"
    else if (indirect_paths_rf$p.value[i] < 0.1) signif <- "."
    
    cat(sprintf("%s (β=%.3f, p=%.4f) %s\n", 
                indirect_paths_rf$term[i],
                indirect_paths_rf$estimate[i], 
                indirect_paths_rf$p.value[i],
                signif))
  }

  # Pour RF - Calcul et affichage des indices d'ajustement moyens
  cat("\n=== INDICES D'AJUSTEMENT MOYENS (RF) ===\n")
  fit_indices_rf <- lapply(fit_rf_list[!sapply(fit_rf_list, is.null)], fitMeasures)
  fit_indices_rf_mean <- Reduce("+", fit_indices_rf) / length(fit_indices_rf)

  cat(sprintf("Chi² moyen = %.3f (df=%.0f)\n", 
              fit_indices_rf_mean["chisq"], 
              fit_indices_rf_mean["df"]))
  cat(sprintf("CFI moyen = %.3f\n", fit_indices_rf_mean["cfi"]))
  cat(sprintf("TLI moyen = %.3f\n", fit_indices_rf_mean["tli"]))
  cat(sprintf("RMSEA moyen = %.3f\n", fit_indices_rf_mean["rmsea"]))
  cat(sprintf("SRMR moyen = %.3f\n", fit_indices_rf_mean["srmr"]))
}

# Légende des niveaux de significativité
cat("\n\nLégende des niveaux de significativité:\n")
cat("*** p<0.001, ** p<0.01, * p<0.05, . p<0.1\n")
```

## Analyse des résultats - Imputation RF

L'analyse utilisant l'imputation par Random Forest (RF) présente des caractéristiques intermédiaires entre l'analyse sur cas complets et l'imputation PMM, tout en maintenant une forte cohérence des effets.

### Ajustement du modèle

Le modèle RF présente un ajustement satisfaisant : - Le CFI moyen (0.907) dépasse le seuil recommandé de 0.90 - Le TLI moyen (0.845) s'améliore par rapport au modèle noDM mais reste inférieur au seuil de 0.90 - Le RMSEA moyen (0.103) reste élevé, proche de celui du modèle noDM - Le SRMR moyen (0.069) est bien dans la zone d'acceptabilité

L'ajustement est meilleur que celui des données complètes mais légèrement moins bon que celui de l'imputation PMM.

### Comparaison des coefficients

La cohérence entre les trois approches est robuste : - Le coefficient CredME → Image (0.481) est quasiment identique à celui obtenu avec PMM (0.482) - L'effet Note → Image (0.336) est également identique entre RF et PMM - La relation CredE → CredME est légèrement plus forte avec RF (0.514 vs 0.481), tandis que CredGD → CredME est légèrement plus faible (0.272 vs 0.298) - Les effets indirects conservent leur importance relative à travers les trois méthodes

## Synthèse comparative et analyse de sensibilité

La comparaison des trois méthodes (cas complets, PMM, RF) constitue une analyse de sensibilité robuste qui renforce considérablement la fiabilité de nos résultats. Les points essentiels à retenir sont :

1.  **Stabilité des effets principaux** : Les relations clés (Note → Image, CredME → Image, Source → CredE) sont constantes à travers les trois approches, avec des variations minimales dans les coefficients standardisés.

2.  **Ajustement amélioré avec imputation** : Les deux méthodes d'imputation, particulièrement PMM, améliorent l'ajustement du modèle, suggérant que le traitement des données manquantes renforce la validité de nos conclusions.

3.  **Convergence des médiations** : Les chemins de médiation significatifs restent cohérents à travers les trois analyses, confirmant le rôle crucial de :

    -   L'effet indirect de la source sur l'image via la crédibilité (Source → CredE → CredME → Image)
    -   L'effet indirect de la note via la congruence et les mécanismes de crédibilité

Cette triple validation confirme la robustesse de notre modèle conceptuel, démontrant que les relations identifiées ne sont pas sensibles à la méthode de traitement des données manquantes. La concordance entre les trois approches suggère que nos conclusions reflètent des effets réels plutôt que des artefacts méthodologiques.

Une analyse de sensibilité supplémentaire ne semble pas nécessaire, car la comparaison entre ces trois méthodes distinctes constitue déjà une validation croisée solide de nos résultats. La robustesse de ces résultats est confirmée par leur stabilité à travers trois méthodes d'analyse différentes (données complètes, PMM, RF), renforçant ainsi la validité de nos conclusions.

# 5. Validation des hypothèses

## Synthèse des résultats et validation des hypothèses

Sur la base des analyses SEM réalisées, nous pouvons valider ou infirmer nos hypothèses de recherche :

### Relations directes

**H1**: La note Glassdoor de l'employeur (Note) influence positivement l'image employeur (Image). - ✓ VALIDÉE - Effet direct significatif (β = 0.336, p \< 0.001) dans les deux méthodes d'imputation - Relation forte et stable

**H2**: La note Glassdoor de l'employeur (Note) influence positivement la congruence des signaux (Cong). - ✓ VALIDÉE - Effet direct fort et significatif (β = 0.617, p \< 0.001) dans les deux méthodes - Relation très forte et stable

**H3**: La source de l'enquête (Source) influence positivement la congruence des signaux (Cong). - ✗ NON VALIDÉE - Effet non significatif (β = 0.025, p = 0.524) dans les deux méthodes - Relation faible et non significative

**H4**: La note Glassdoor de l'employeur (Note) influence la crédibilité perçue de la note Glassdoor de l'employeur (CredGD). - ✗ NON VALIDÉE - Effet direct non significatif (β = 0.098, p = 0.133) dans les deux méthodes - L'influence passe principalement par la congruence

**H5**: La source de l'enquête (Source) influence la crédibilité perçue de l'enquête (CredE). - ✓ VALIDÉE - Effet direct significatif (β ≈ 0.35, p \< 0.001) dans les deux méthodes - Relation stable et forte

**H6**: La congruence des signaux (Cong) influence positivement la crédibilité des signaux. - ✓ VALIDÉE - H6a: Effet significatif sur CredE (β ≈ 0.25, p \< 0.001) dans les deux méthodes - H6b: Effet significatif sur CredGD (β = 0.228, p \< 0.001) dans les deux méthodes

**H7**: La crédibilité de la marque employeur (CredME) influence positivement l'image employeur (Image). - ✓ VALIDÉE - Effet direct fort (β ≈ 0.48, p \< 0.001) dans les deux méthodes - Impact le plus important sur l'image dans le modèle

**H8**: La crédibilité des signaux influence la crédibilité de la marque employeur (CredME). - ✓ VALIDÉE - H8a: CredGD → CredME significatif (β ≈ 0.27-0.30, p \< 0.001) dans les deux méthodes - H8b: CredE → CredME significatif (β ≈ 0.49-0.51, p \< 0.001) dans les deux méthodes

### Relations de médiation

**H9**: Les effets de la source de l'enquête sur l'image employeur sont médiatisés par une chaîne de médiation impliquant la congruence des signaux, la crédibilité des signaux et la crédibilité de la marque employeur. - ✗ NON VALIDÉE - H9a: Chaîne Source → Cong → CredGD → CredME → Image non significative (ind2 ≈ 0.001, p \> 0.54) - H9b: Chaîne Source → Cong → CredE → CredME → Image non significative (ind1 ≈ 0.001-0.002, p \> 0.53) - L'absence d'effet significatif de Source sur Cong empêche ces chaînes de médiation d'être significatives

**H10**: Les effets de la source de l'enquête sur l'image employeur sont médiatisés par une chaîne de médiation impliquant la crédibilité de l'enquête et la crédibilité de la marque employeur. - ✓ VALIDÉE - Effet indirect significatif Source → CredE → CredME → Image (ind3 ≈ 0.085, p \< 0.001) dans les deux méthodes - Représente le principal chemin par lequel la source influence l'image

**H11**: Les effets de la note Glassdoor sur l'image employeur sont médiatisés par une chaîne de médiation impliquant la crédibilité de la note Glassdoor et la crédibilité de la marque employeur. - ✗ NON VALIDÉE - Effet indirect Note → CredGD → CredME → Image non significatif (ind6 ≈ 0.013-0.014, p ≈ 0.14) - L'absence d'effet significatif de Note sur CredGD empêche cette chaîne de médiation d'être significative

**H12**: Les effets de la note sur l'image employeur sont médiatisés par une chaîne de médiation impliquant la congruence des signaux, la crédibilité des signaux et la crédibilité de la marque employeur. - ✓ VALIDÉE - H12a: Chaîne Note → Cong → CredGD → CredME → Image significative (ind4 ≈ 0.019, p \< 0.01) - H12b: Chaîne Note → Cong → CredE → CredME → Image significative (ind5 ≈ 0.037, p \< 0.001) - Effet indirect total significatif (tot_ind = 0.157, p \< 0.001)

### Conclusion

Les analyses SEM basées sur les méthodes d'imputation PMM et RF révèlent un support empirique solide pour la majorité de nos hypothèses, démontrant la complexité des mécanismes par lesquels la note d'employeur et sa source influencent l'image de l'entreprise. Nos résultats mettent en évidence trois points majeurs :

1.  **Le rôle central de la crédibilité dans la formation de l'image employeur** :
    -   La crédibilité de la marque employeur comme déterminant principal de l'image
    -   L'influence significative des différentes formes de crédibilité (CredE, CredGD, CredME)
2.  **L'importance des mécanismes de médiation** :
    -   La congruence des signaux comme médiateur clé pour les effets de la note
    -   La crédibilité de l'enquête comme médiateur principal pour les effets de la source
    -   Les chaînes de médiation multiples impliquant la crédibilité
3.  **La différenciation des effets selon la source et la note** :
    -   Impact direct et indirect de la note sur l'image
    -   Absence d'effet de la source sur la congruence, contrairement à l'hypothèse initiale
    -   Importance de la congruence dans la transmission des effets de la note, mais pas de la source

Ces résultats sont particulièrement robustes car ils sont cohérents entre les deux méthodes d'imputation (PMM et RF), ce qui renforce considérablement la fiabilité de nos conclusions.
